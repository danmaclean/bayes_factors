[["index.html", "Bayesian Inference with Bayes Factors Topic 1 Setting up 1.1 Prerequisites 1.2 Installing R 1.3 Installing RStudio 1.4 Installing R packages in RStudio", " Bayesian Inference with Bayes Factors Dan MacLean 2021-02-10 Topic 1 Setting up The primary purpose of this course is to help you to understand how to use statistics that will help with your research. The course will try to explain a branch of statistics called ‘Estimation Statistics’ which are complementary to the normal sort of hypothesis test procedures and address some of the criticisms of those methods. Statistics is a computationally heavy topic, so we’ll be making use of the R statistical programming environment to do that side of the work. The rest of this chapter will help you get that set up on your own computer. 1.1 Prerequisites 1.1.1 Knowledge prerequisites There are no specific knowledge prerequisites for this book but it will be very helpful if you have read and worked through the ggplot, Intro to Stats and Estimation Statistics books and are familiar with R use. 1.1.2 Software prerequisites You need to install the following stuff for this book: R RStudio Some R packages: tidyverse and BayesFactor 1.2 Installing R Follow this link and install the right version for your operating system https://www.stats.bris.ac.uk/R/ 1.3 Installing RStudio Follow this link and install the right version for your operating system https://www.rstudio.com/products/rstudio/download/ 1.4 Installing R packages in RStudio 1.4.1 Standard packages In the RStudio console, type install.packages(c(\"tidyverse\", \"BayesFactor\")) and the packages should install. "],["motivation.html", "Topic 2 Motivation", " Topic 2 Motivation The sort of statistics that most experimental science students are taught are called ‘Frequentist Statistics’. They include the \\(t\\)-tests, ANOVA and \\(\\chi^2\\)-tests and the linear models that we have studied already. The inferential approach in the Frequentist paradigm is often criticised for being weak and is often abused. Although the abuse is as much a consequence of convention in the scientific literature and in scientific publishing, the misinterpretation of \\(p\\)-values by generations of scientists as it is the philosophical weakness of the methods themselves, the weaknesses persist and over time other paradigms have emerged. We have seen an alternative in Estimation Statistics, in this book we will look at another - Bayesian Inference and using Bayes Factors to compare levels of evidence for one hypothesis over another, rather than just accepting or rejecting a simplistic null hypothesis. The advantage of this will be that we can much more directly select between specific hypotheses that might describe our data. This will give us a much clearer idea about a question that we instinctively want to answer when we do statistics - ‘Which hypothesis is most likely true?’, we will see that we can formulate this in lots of ways, but in general the hypotheses we want to compare will be something along the lines of some measured quantity being different in different samples. With Frequentist Inference we can only ask the roundabout question, ‘How often does the difference we observe occur by chance?’ and if it isn’t likely, say so. With Bayes Factors we will be able to compare directly competing hypotheses and reject the least likely absolutely. "],["r-fundamentals.html", "Topic 3 R Fundamentals 3.1 About this chapter 3.2 Working with R 3.3 Variables 3.4 Dataframes 3.5 Packages 3.6 Using R Help", " Topic 3 R Fundamentals 3.1 About this chapter Questions: How do I use R? Objectives: Become familiar with R syntax Understand the concepts of objects and assignment Get exposed to a few functions Keypoints: R’s capabilities are provided by functions R users call functions and get results 3.2 Working with R In this workshop we’ll use R in the extremely useful RStudio software. For the most part we’ll work interactively, meaning we’ll type stuff straight into the R console in RStudio (Usually this is a window on the left or lower left) and get our results there too (usually in the console or in a window on the right). Panels like the ones below mimic the interaction with R and first show the thing to type into R, and below the calculated result from R. Let’s look at how R works by using it for it’s most basic job - as a calculator: 3 + 5 ## [1] 8 12 * 2 ## [1] 24 1 / 3 ## [1] 0.3333333 12 * 2 ## [1] 24 Fairly straightforward, we type in the expression and we get a result. That’s how this whole book will work, you type the stuff in, and get answers out. It’ll be easiest to learn if you go ahead and copy the examples one by one. Try to resist the urge to use copy and paste. Typing longhand really encourages you to look at what you’re entering. As far as the R output itself goes, it’s really straightforward - its just the answer with a [1] stuck on the front. This [1] tells us how many items through the output we are. Often R will return long lists of numbers and it can be helpful to have this extra information. 3.3 Variables We can save the output of operations for later use by giving it a name using the assignment symbol &lt;-. Read this symbol as ‘gets’, so x &lt;- 5 reads as ‘x gets 5’. These names are called variables, because the value they are associated with can change. Let’s give five a name, x then refer to the value 5 by it’s name. We can then use the name in place of the value. In the jargon of computing we say we are assigning a value to a variable. x &lt;- 5 x ## [1] 5 x * 2 ## [1] 10 y &lt;- 3 x * y ## [1] 15 This is of course of limited value with just numbers but is of great value when we have large datasets, as the whole thing can be referred to by the variable. 3.3.1 Using objects and functions At the top level, R is a simple language with two types of thing: functions and objects. As a user you will use functions to do stuff, and get back objects as an answer. Functions are easy to spot, they are a name followed by a pair of brackets. A function like mean() is the function for calculating a mean. The options (or arguments) for the function go inside the brackets: sqrt(16) ## [1] 4 Often the result from a function will be more complicated than a simple number object, often it will be a vector (simple list), like from the rnorm() function that returns lists of random numbers rnorm(100) ## [1] 0.466976714 -0.078836515 1.059462746 0.697027766 -0.105890203 ## [6] 0.666049590 0.705126822 -0.001082432 0.078698249 0.472373976 ## [11] -1.375410915 0.752280788 1.328029185 -0.511143937 0.923039539 ## [16] -0.721621339 2.529702074 -1.160885864 1.394615747 -0.026885777 ## [21] -1.150023371 -0.459615523 -0.072355869 0.501046886 -0.303849342 ## [26] 2.169446346 0.884288637 0.825005089 -0.306634388 1.646726004 ## [31] 0.745004364 -1.022861719 1.130412668 -0.843503879 1.867249747 ## [36] -0.596582720 -0.918953261 0.262367150 -0.470602588 -0.335903426 ## [41] 0.835705663 2.348058135 -0.315145614 -0.688151908 1.091165216 ## [46] 1.178224245 -0.321773584 -1.073670255 0.996495337 -0.228247870 ## [51] -0.498091765 -0.555484314 -1.138013496 -0.859852767 -1.391040920 ## [56] 0.205431910 0.667499339 0.893763961 -0.777135631 -0.209615641 ## [61] -1.325872475 -0.001269401 -0.818527384 -0.931759506 -0.540920791 ## [66] -0.040368975 0.754292276 -0.892980991 -0.264293648 -1.487470746 ## [71] -0.857940479 0.790638619 -0.031036179 0.258852522 -2.384464229 ## [76] 0.758021812 -1.608371857 -1.846592721 0.579894879 0.092127108 ## [81] 1.079978151 1.109559124 1.505198044 -1.384923779 1.474869593 ## [86] -0.003946462 -1.165096605 0.271659246 0.620935469 -2.055654478 ## [91] -0.384625634 -0.537313717 -0.953703023 0.809877482 -0.839522337 ## [96] 1.656822011 -0.020958200 0.370761608 1.453588492 -0.662058895 We can combine objects, variables and functions to do more complex stuff in R, here’s how we get the mean of 100 random numbers. numbers &lt;- rnorm(100) mean(numbers) ## [1] 0.1857514 Here we created a vector object with rnorm(100) and assigned it to the variable numbers. We than used the mean() function, passing it the variable numbers. The mean() function returned the mean of the hundred random numbers. 3.4 Dataframes One of the more common objects that R uses is a dataframe. The dataframe is a rectangular table-like object that contains data, think of it like a spreadsheet tab. Like the spreadsheet, the dataframe has rows and columns, the columns have names and the different columns can have different types of data in. Here’s a little one ## names age score ## 1 Guido 24 17.57791 ## 2 Marty 45 38.10125 ## 3 Alan 11 39.37160 Usually we get a dataframe by loading in data from an external source or as a result from functions, occasionally we’ll want to hand make one, which can be done with various functions, data.frame being the most common. data.frame( names = c(&quot;Guido&quot;, &quot;Marty&quot;, &quot;Alan&quot;), age = c(24,45,11), score = runif(3) * 100 ) 3.5 Packages Many of the tools we use in will come in R packages, little nuggets of code that group related functions together. Installing new packages can be done using the Packages pane of RStudio or the install.packages() function. When we wish to use that code we use the library() function library(somepackage) 3.6 Using R Help R provides a command, called ? that will display the documentation for functions. For example ?mean will display the help for the mean() function. ?mean As in all programming languages the internal documentation in R is written with some assumption that the reader is familiar with the language. This can be a pain when you are starting out as the help will seem a bit obscure at times. Don’t worry about this, usually the Examples section will give you a good idea of how to use the function and as your experience grows then the more things will make more sense. * R is an excellent and powerful statistical computing environment Complete the interactive tutorial online https://danmaclean.shinyapps.io/r-start "],["bayesian-inference.html", "Topic 4 Bayesian Inference 4.1 Frequentist and Bayesian Interpretations of Probability 4.2 Bayes Theorem by Rough Example 4.3 Hypotheses in Frequentist and Bayesian Statistics 4.4 Bayes Factors", " Topic 4 Bayesian Inference 4.1 Frequentist and Bayesian Interpretations of Probability It may seem like a strange question to ask, but what, exactly, is probability? Whatever it is it certainly isn’t a solid thing that we could carry in a bucket. Probability is a strange and often ill-defined concept that can get very confusing when one starts to think deeply about it. When asked what probability is people will generally start to talk about vague concepts like chance or likelihood or randomness or fate, even. Most people will give examples of coins being thrown or dice being rolled. This ephemerality is no good when we want to use probability so when it comes to working with probability statisticians needed to develop very precise definitions. It turns out that different ways of thinking about likelihoods can result in very different definitions of probability. The two definitions that we will consider are those called the Frequentist and the Bayesian definitions 4.1.1 Frequentist Probability The Frequentist definition of probability is based on the frequency of occurrence of events. This is a definition that is most similar to the coin toss or dice throw intuition about probability. A probability can be stated thus \\(P(Event) = \\frac{\\text{number of ways event can happen}}{\\text{number of all possible outcomes}}\\) So in a coin toss, we might get the following probability of getting ‘heads’ \\(P(heads) = \\frac{\\text{number of heads on the coin}}{\\text{number of sides to the coin}}\\) which of course, computes as \\(P(heads) = \\frac{1}{2}\\) Thinking of probabilities in this way is similar to a gambler who plays games of chance like roulette or craps, where the odds of winning are entirely based on the outcome of simple random process. This is so simple and intuitive that we might be tempted to think it’s the natural way to think about probabilities, but there are other definitions. 4.1.2 Bayesian Probablity The Bayesian definition of probability is different, it takes probability to be a reasonable expectation of an event, depending on the knowledge that the observer has. You might understand these probabilities similarly to a gambler that bets on horse races and changes their assessment of a horse’s winning ability based on the conditions of the ground and the weight of the jockey. These are trickier to understand than the Frequentist definition but an example can be helpful. Consider that you and a friend are playing cards and that your friend claims to be able to guess the identity of a card that you draw and replace. A frequentist probability would say that the probability of this was \\(P(correct) = \\frac{1}{52}\\). However, you know that your friend is an amateur magician, so you expect that the probability of a correct guess would be much higher than that. That is to say that you have a different reasonable expectation because you have incorporated prior knowledge into your working. Bayesian Probability is based on this prior knowledge and updating of belief based on that knowledge to come up with a posterior likelihood of an event. In rough terms the answer - a ‘posterior probability’ is arrived at by combining a ‘prior probability’ and ‘evidence’. In the card guess example the ‘prior probability’ was the raw chance based probability that anyone would guess the card \\(\\frac{1}{52}\\), the ‘evidence’ was the fact that your friend was an amateur magician and the ‘posterior probability’ was the updated ‘prior probability’ that the chance of guessing was higher than \\(\\frac{1}{52}\\). One problem we might spot is how exactly do we update our probability to actually get a measure of the posterior? A formula known as Bayes Theorem lets us do the calculation, but it can be very hard to get the actual numbers we need for evidence and this can be a barrier to using Bayes in the real world. However, let’s look work one calculation through with some assumed numbers to get a feel. 4.2 Bayes Theorem by Rough Example The mathematical basis of calculating a posterior belief or likelihood is done with a formula called Bayes Theorem. Which, using our card example defines the posterior as \\(P(correct | magician)\\) which reads as the probability of a guess being correct once you know you are working with a magician. It defines the prior as \\(P(correct)\\) which reads as the probability of being correct in a random guess (which we know to be \\(\\frac{1}{52}\\)) And it defines the evidence as \\(P(magician|correct)\\) which reads as the probability of the person being a magician given a guess was correct. This is the number which can be hardest to work out in general though in this case we might say it is quite high, say 0.9. Bayes Theorem then works out the posterior probability given these numbers. There is a very famous formula for this, that I won’t include here for simplicity sake, but it is very interesting. We can take a short cut and use R to work out the posterior from the prior and the evidence as follows library(LaplacesDemon) prior &lt;- c(51/52,1/52) evidence &lt;- c(0.9, 0.1) BayesTheorem(prior, evidence) ## [1] 0.997826087 0.002173913 ## attr(,&quot;class&quot;) ## [1] &quot;bayestheorem&quot; as it is the first reported number we want, we can see that we get a 99% posterior probability that the guess will be correct if we know that the 90% of correct guesser’s are magicians. The key thing to take away here is that the Bayesian Probability allows us to modify our view based on changes in the evidence. This is a key attribute as we can use it to compare the resulting posteriors from different evidences. In other words it allows us to compare different hypotheses based on different evidence to see which is the more likely. 4.3 Hypotheses in Frequentist and Bayesian Statistics Now that we know Bayes Statistics allow for updating our beliefs in the light of different evidence we can look at how we can formulate hypotheses to take advantage of this and do something very different with Bayes than we do with Frequentist ideas. Let’ recap the logic of hypothesis tests in Frequentist statistics. 4.3.1 Frequentist Hypotheses You may recall that the first step of doing a hypothesis test like a \\(t\\)-test is to set up our hypotheses. The first \\(H_0\\) is the null hypothesis which represents the situation where there is no difference and \\(H_1\\) is the alternative. Next we select a Null model that represents the Null hypothesis, this step is usually implicit at the operator level and comes as part of the linear model or \\(t\\)-test that we choose to use, and usually is based on the Normal Distribution. Our hypothesis represent the situation as follows \\(H_0 : \\bar{x}_1 - \\bar{x}_2 = 0\\) IE, the sample means are equal. \\(H_1 : \\bar{x}_2 - \\bar{x}_2 \\neq 0\\) IE, the sample means are not equal. We test \\(H_0\\) (the Null Hypothesis and Model) to see how likely the observed result is under that and if it is unlikely at some level (\\(p\\)) then we reject \\(H_0\\) and accept \\(H_1\\). We criticised this for being weak inference in the Linear Model course. Let’s do that again. In this framework haven’t we accepted \\(H_1\\) without analysing it? Here it means that we have had to set up hypotheses that are binary and not compare them directly. We have a take or leave approach to hypotheses. We haven’t, for example been able to ask whether \\(\\bar{x}_1 &gt; \\bar{x}_2\\) because that wouldn’t be askable under our single test, binary paradigm. That’s a limitation. As scientists we should be able to collect data and compare models or hypotheses about that data directly. 4.3.2 Bayesian Hypotheses In the Bayesian Framework we can formulate hypotheses as we wish and compare them directly, using Bayesian probabilities to examine models with different evidences and priors. So if the evidence shows that \\(H_1\\) isn’t any more believable than \\(H_0\\) we wouldn’t falsely fall into the trap of believing \\(H_1\\) was somehow more correct. Bayesian Hypotheses can be a bit more like this \\(H_0 : \\bar{x}_1 &lt; \\bar{x}_2\\) IE sample 1 has a lower mean than sample 2 \\(H_1 : \\bar{x}_1 &gt; \\bar{x}_2\\) IE sample 1 has a higher mean than sample 2. which is often much more intellectually satisfying and can lead to clearer answers than the more binary Frequentist hypotheses. A significant limitation of the approach is the need to select and quantify the prior and the evidence, which can be crucial and lead to very different outcomes if different values are chosen. Selection of the prior knowledge itself is very difficult and no suitable data may exist. Getting the right data is subjective in many cases and there is no one right way. Domain knowledge is important and often crucial but this can easily lead to bias. An unwitting, uncareful (or say it quietly - unscrupulous) operator could select a prior that would bias the result in favour of a preferred hypothesis. This is a form of confirmation bias or interpretation of the data in a way that confirms your prior beliefs. For these reasons Frequentist approaches are often the most pragmatic and a priori transparent method, though if the priors and evidence can be collected in a non-biased way Bayesian approaches offer us excellent alternatives. 4.4 Bayes Factors We can use Bayesian Inference through a tool known as Bayes Factors. Bayes Factors are a method of directly comparing the posteriors of different models with different evidences and priors. Bayes Factors make a ratio of the result of one model or hypothesis over another, resulting in a single quantity that we can examine. Consider that our hypotheses above have been put through the process and a result gained thus \\(H_0 : \\bar{x}_1 &lt; \\bar{x}_2 = 0.3\\) \\(H_1 : \\bar{x}_1 &gt; \\bar{x}_2 = 0.9\\) We can clearly see that \\(H_1\\) has 3 times more support than \\(H_0\\) and we would want to accept that as a better explanation of our data. Bayes Factors are just that, the ratio of the relative goodness of the hypotheses. From this we can make statements about the support for hypotheses. Wagenmakers et al. (2011) created a table of thresholds indicating interpretations for different Bayes Factors on two hypotheses. Bayes.Factor Interpretation &gt;100 Extreme evidence for \\(H_0\\) compared to \\(H_1\\) 30..100 Very Strong evidence for \\(H_0\\) compared to \\(H_1\\) 10..30 Strong evidence for \\(H_0\\) compared to \\(H_1\\) 3..10 Substantial evidence for \\(H_0\\) compared to \\(H_1\\) 1..3 Anecdotal evidence for \\(H_0\\) compared to \\(H_1\\) 1 No evidence 1..1/3 Anecdotal evidence for \\(H_1\\) compared to \\(H_0\\) 1/3..1/10 Substantial evidence for \\(H_1\\) compared to \\(H_0\\) 1/10..1/30 Strong evidence for \\(H_1\\) compared to \\(H_0\\) 1/30..1/100 Very Strong evidence for \\(H_1\\) compared to \\(H_0\\) &lt;1/100 Extreme evidence for \\(H_1\\) compared to \\(H_0\\) These are extremely useful especially when used with other measures and interpretations like estimation statistics to allow us to make statistical claims. In the next chapters we will look at how to use Bayes Factors in place of common frequentist hypothesis tests. The Wagenmakers et al. (2011) article is fun if you can get hold of it. It’s a commentary on an earlier article in which the researchers conclude that people have the ability to see into the future by misunderstanding and misapplying statistics. Wagenmakers et al reperform the statistics with Bayes Factors and show that the original conclusions are unsound. References "],["bayes-factor-t-tests.html", "Topic 5 Bayes Factor \\(t\\)-tests", " Topic 5 Bayes Factor \\(t\\)-tests "],["bayes-factor-anova.html", "Topic 6 Bayes Factor ANOVA", " Topic 6 Bayes Factor ANOVA "],["bayes-factor-chi2.html", "Topic 7 Bayes Factor \\(\\chi^2\\)", " Topic 7 Bayes Factor \\(\\chi^2\\) "],["references.html", "References", " References "]]
