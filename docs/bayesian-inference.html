<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 4 Bayesian Inference | Bayesian Inference with Bayes Factors</title>
  <meta name="description" content="Topic 4 Bayesian Inference | Bayesian Inference with Bayes Factors" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 4 Bayesian Inference | Bayesian Inference with Bayes Factors" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 4 Bayesian Inference | Bayesian Inference with Bayes Factors" />
  
  
  

<meta name="author" content="Dan MacLean" />


<meta name="date" content="2021-02-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="r-fundamentals.html"/>
<link rel="next" href="bayes-factor-t-tests.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Bayesian Inference Primer</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Setting up</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#knowledge-prerequisites"><i class="fa fa-check"></i><b>1.1.1</b> Knowledge prerequisites</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#software-prerequisites"><i class="fa fa-check"></i><b>1.1.2</b> Software prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#installing-r"><i class="fa fa-check"></i><b>1.2</b> Installing R</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#installing-rstudio"><i class="fa fa-check"></i><b>1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#installing-r-packages-in-rstudio"><i class="fa fa-check"></i><b>1.4</b> Installing R packages in RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#standard-packages"><i class="fa fa-check"></i><b>1.4.1</b> Standard packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i><b>2</b> Motivation</a></li>
<li class="chapter" data-level="3" data-path="r-fundamentals.html"><a href="r-fundamentals.html"><i class="fa fa-check"></i><b>3</b> R Fundamentals</a><ul>
<li class="chapter" data-level="3.1" data-path="r-fundamentals.html"><a href="r-fundamentals.html#about-this-chapter"><i class="fa fa-check"></i><b>3.1</b> About this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="r-fundamentals.html"><a href="r-fundamentals.html#working-with-r"><i class="fa fa-check"></i><b>3.2</b> Working with R</a></li>
<li class="chapter" data-level="3.3" data-path="r-fundamentals.html"><a href="r-fundamentals.html#variables"><i class="fa fa-check"></i><b>3.3</b> Variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="r-fundamentals.html"><a href="r-fundamentals.html#using-objects-and-functions"><i class="fa fa-check"></i><b>3.3.1</b> Using objects and functions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="r-fundamentals.html"><a href="r-fundamentals.html#dataframes"><i class="fa fa-check"></i><b>3.4</b> Dataframes</a></li>
<li class="chapter" data-level="3.5" data-path="r-fundamentals.html"><a href="r-fundamentals.html#packages"><i class="fa fa-check"></i><b>3.5</b> Packages</a></li>
<li class="chapter" data-level="3.6" data-path="r-fundamentals.html"><a href="r-fundamentals.html#using-r-help"><i class="fa fa-check"></i><b>3.6</b> Using R Help</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>4</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#frequentist-and-bayesian-interpretations-of-probability"><i class="fa fa-check"></i><b>4.1</b> Frequentist and Bayesian Interpretations of Probability</a><ul>
<li class="chapter" data-level="4.1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#frequentist-probability"><i class="fa fa-check"></i><b>4.1.1</b> Frequentist Probability</a></li>
<li class="chapter" data-level="4.1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-probablity"><i class="fa fa-check"></i><b>4.1.2</b> Bayesian Probablity</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayes-theorem-by-rough-example"><i class="fa fa-check"></i><b>4.2</b> Bayes Theorem by Rough Example</a></li>
<li class="chapter" data-level="4.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#hypotheses-in-frequentist-and-bayesian-statistics"><i class="fa fa-check"></i><b>4.3</b> Hypotheses in Frequentist and Bayesian Statistics</a><ul>
<li class="chapter" data-level="4.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#frequentist-hypotheses"><i class="fa fa-check"></i><b>4.3.1</b> Frequentist Hypotheses</a></li>
<li class="chapter" data-level="4.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-hypotheses"><i class="fa fa-check"></i><b>4.3.2</b> Bayesian Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayes-factors"><i class="fa fa-check"></i><b>4.4</b> Bayes Factors</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayes-factor-t-tests.html"><a href="bayes-factor-t-tests.html"><i class="fa fa-check"></i><b>5</b> Bayes Factor <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="6" data-path="bayes-factor-anova.html"><a href="bayes-factor-anova.html"><i class="fa fa-check"></i><b>6</b> Bayes Factor ANOVA</a></li>
<li class="chapter" data-level="7" data-path="bayes-factor-chi2.html"><a href="bayes-factor-chi2.html"><i class="fa fa-check"></i><b>7</b> Bayes Factor <span class="math inline">\(\chi^2\)</span></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference with Bayes Factors</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-inference" class="section level1">
<h1><span class="header-section-number">Topic 4</span> Bayesian Inference</h1>
<div id="frequentist-and-bayesian-interpretations-of-probability" class="section level2">
<h2><span class="header-section-number">4.1</span> Frequentist and Bayesian Interpretations of Probability</h2>
<p>It may seem like a strange question to ask, but what, exactly, is probability? Whatever it is it certainly isn’t a solid thing that we could carry in a bucket. Probability is a strange and often ill-defined concept that can get very confusing when one starts to think deeply about it. When asked what probability is people will generally start to talk about vague concepts like chance or likelihood or randomness or fate, even. Most people will give examples of coins being thrown or dice being rolled. This ephemerality is no good when we want to use probability so when it comes to working with probability statisticians needed to develop very precise definitions. It turns out that different ways of thinking about likelihoods can result in very different definitions of probability.</p>
<p>The two definitions that we will consider are those called the Frequentist and the Bayesian definitions</p>
<div id="frequentist-probability" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Frequentist Probability</h3>
<p>The Frequentist definition of probability is based on the frequency of occurrence of events. This is a definition that is most similar to the coin toss or dice throw intuition about probability. A probability can be stated thus</p>
<p><span class="math inline">\(P(Event) = \frac{\text{number of ways event can happen}}{\text{number of all possible outcomes}}\)</span></p>
<p>So in a coin toss, we might get the following probability of getting ‘heads’</p>
<p><span class="math inline">\(P(heads) = \frac{\text{number of heads on the coin}}{\text{number of sides to the coin}}\)</span></p>
<p>which of course, computes as</p>
<p><span class="math inline">\(P(heads) = \frac{1}{2}\)</span></p>
<p>Thinking of probabilities in this way is similar to a gambler who plays games of chance like roulette or craps, where the odds of winning are entirely based on the outcome of simple random process.</p>
<p>This is so simple and intuitive that we might be tempted to think it’s the natural way to think about probabilities, but there are other definitions.</p>
</div>
<div id="bayesian-probablity" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Bayesian Probablity</h3>
<p>The Bayesian definition of probability is different, it takes probability to be a reasonable expectation of an event, depending on the knowledge that the observer has. You might understand these probabilities similarly to a gambler that bets on horse races and changes their assessment of a horse’s winning ability based on the conditions of the ground and the weight of the jockey. These are trickier to understand than the Frequentist definition but an example can be helpful.</p>
<p>Consider that you and a friend are playing cards and that your friend claims to be able to guess the identity of a card that you draw and replace. A frequentist probability would say that the probability of this was <span class="math inline">\(P(correct) = \frac{1}{52}\)</span>. However, you know that your friend is an amateur magician, so you expect that the probability of a correct guess would be much higher than that. That is to say that you have a different reasonable expectation because you have incorporated prior knowledge into your working. Bayesian Probability is based on this prior knowledge and updating of belief based on that knowledge to come up with a posterior likelihood of an event.</p>
<p>In rough terms the answer - a ‘posterior probability’ is arrived at by combining a ‘prior probability’ and ‘evidence’. In the card guess example the ‘prior probability’ was the raw chance based probability that anyone would guess the card <span class="math inline">\(\frac{1}{52}\)</span>, the ‘evidence’ was the fact that your friend was an amateur magician and the ‘posterior probability’ was the updated ‘prior probability’ that the chance of guessing was higher than <span class="math inline">\(\frac{1}{52}\)</span>.</p>
<p>One problem we might spot is how exactly do we update our probability to actually get a measure of the posterior? A formula known as Bayes Theorem lets us do the calculation, but it can be very hard to get the actual numbers we need for evidence and this can be a barrier to using Bayes in the real world. However, let’s look work one calculation through with some assumed numbers to get a feel.</p>
</div>
</div>
<div id="bayes-theorem-by-rough-example" class="section level2">
<h2><span class="header-section-number">4.2</span> Bayes Theorem by Rough Example</h2>
<p>The mathematical basis of calculating a posterior belief or likelihood is done with a formula called Bayes Theorem. Which, using our card example defines the posterior as</p>
<p><span class="math inline">\(P(correct | magician)\)</span></p>
<p>which reads as the probability of a guess being correct once you know you are working with a magician.</p>
<p>It defines the prior as</p>
<p><span class="math inline">\(P(correct)\)</span></p>
<p>which reads as the probability of being correct in a random guess (which we know to be <span class="math inline">\(\frac{1}{52}\)</span>)</p>
<p>And it defines the evidence as</p>
<p><span class="math inline">\(P(magician|correct)\)</span></p>
<p>which reads as the probability of the person being a magician given a guess was correct. This is the number which can be hardest to work out in general though in this case we might say it is quite high, say 0.9.</p>
<p>Bayes Theorem then works out the posterior probability given these numbers. There is a very famous formula for this, that I won’t include here for simplicity sake, but it is very interesting. We can take a short cut and use R to work out the posterior from the prior and the evidence as follows</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="bayesian-inference.html#cb26-1"></a><span class="kw">library</span>(LaplacesDemon)</span>
<span id="cb26-2"><a href="bayesian-inference.html#cb26-2"></a>prior &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">51</span><span class="op">/</span><span class="dv">52</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">52</span>) </span>
<span id="cb26-3"><a href="bayesian-inference.html#cb26-3"></a>evidence &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.9</span>, <span class="fl">0.1</span>)</span>
<span id="cb26-4"><a href="bayesian-inference.html#cb26-4"></a></span>
<span id="cb26-5"><a href="bayesian-inference.html#cb26-5"></a><span class="kw">BayesTheorem</span>(prior, evidence)</span></code></pre></div>
<pre><code>## [1] 0.997826087 0.002173913
## attr(,&quot;class&quot;)
## [1] &quot;bayestheorem&quot;</code></pre>
<p>as it is the first reported number we want, we can see that we get a 99% posterior probability that the guess will be correct if we know that the 90% of correct guesser’s are magicians.</p>
<p>The key thing to take away here is that the Bayesian Probability allows us to modify our view based on changes in the evidence. This is a key attribute as we can use it to compare the resulting posteriors from different evidences. In other words it allows us to compare different hypotheses based on different evidence to see which is the more likely.</p>
</div>
<div id="hypotheses-in-frequentist-and-bayesian-statistics" class="section level2">
<h2><span class="header-section-number">4.3</span> Hypotheses in Frequentist and Bayesian Statistics</h2>
<p>Now that we know Bayes Statistics allow for updating our beliefs in the light of different evidence we can look at how we can formulate hypotheses to take advantage of this and do something very different with Bayes than we do with Frequentist ideas.</p>
<p>Let’ recap the logic of hypothesis tests in Frequentist statistics.</p>
<div id="frequentist-hypotheses" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Frequentist Hypotheses</h3>
<p>You may recall that the first step of doing a hypothesis test like a <span class="math inline">\(t\)</span>-test is to set up our hypotheses. The first <span class="math inline">\(H_0\)</span> is the null hypothesis which represents the situation where there is no difference and <span class="math inline">\(H_1\)</span> is the alternative. Next we select a Null model that represents the Null hypothesis, this step is usually implicit at the operator level and comes as part of the linear model or <span class="math inline">\(t\)</span>-test that we choose to use, and usually is based on the Normal Distribution. Our hypothesis represent the situation as follows</p>
<ul>
<li><span class="math inline">\(H_0 : \bar{x}_1 - \bar{x}_2 = 0\)</span> IE, the sample means are equal.</li>
<li><span class="math inline">\(H_1 : \bar{x}_2 - \bar{x}_2 \neq 0\)</span> IE, the sample means are not equal.</li>
</ul>
<p>We test <span class="math inline">\(H_0\)</span> (the Null Hypothesis and Model) to see how likely the observed result is under that and if it is unlikely at some level (<span class="math inline">\(p\)</span>) then we reject <span class="math inline">\(H_0\)</span> and accept <span class="math inline">\(H_1\)</span>.</p>
<p>We criticised this for being weak inference in the Linear Model course. Let’s do that again. In this framework haven’t we accepted <span class="math inline">\(H_1\)</span> without analysing it? Here it means that we have had to set up hypotheses that are binary and not compare them directly. We have a take or leave approach to hypotheses.</p>
<p>We haven’t, for example been able to ask whether <span class="math inline">\(\bar{x}_1 &gt; \bar{x}_2\)</span> because that wouldn’t be askable under our single test, binary paradigm. That’s a limitation. As scientists we should be able to collect data and compare models or hypotheses about that data directly.</p>
</div>
<div id="bayesian-hypotheses" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Bayesian Hypotheses</h3>
<p>In the Bayesian Framework we can formulate hypotheses as we wish and compare them directly, using Bayesian probabilities to examine models with different evidences and priors. So if the evidence shows that <span class="math inline">\(H_1\)</span> isn’t any more believable than <span class="math inline">\(H_0\)</span> we wouldn’t falsely fall into the trap of believing <span class="math inline">\(H_1\)</span> was somehow more correct.</p>
<p>Bayesian Hypotheses can be a bit more like this</p>
<ul>
<li><span class="math inline">\(H_0 : \bar{x}_1 &lt; \bar{x}_2\)</span> IE sample 1 has a lower mean than sample 2</li>
<li><span class="math inline">\(H_1 : \bar{x}_1 &gt; \bar{x}_2\)</span> IE sample 1 has a higher mean than sample 2.</li>
</ul>
<p>which is often much more intellectually satisfying and can lead to clearer answers than the more binary Frequentist hypotheses.</p>
<p>A significant limitation of the approach is the need to select and quantify the prior and the evidence, which can be crucial and lead to very different outcomes if different values are chosen.</p>
<p>Selection of the prior knowledge itself is very difficult and no suitable data may exist. Getting the right data is subjective in many cases and there is no one right way. Domain knowledge is important and often crucial but this can easily lead to bias. An unwitting, uncareful (or say it quietly - unscrupulous) operator could select a prior that would bias the result in favour of a preferred hypothesis. This is a form of confirmation bias or interpretation of the data in a way that confirms your prior beliefs.</p>
<p>For these reasons Frequentist approaches are often the most pragmatic and <em>a priori</em> transparent method, though if the priors and evidence can be collected in a non-biased way Bayesian approaches offer us excellent alternatives.</p>
</div>
</div>
<div id="bayes-factors" class="section level2">
<h2><span class="header-section-number">4.4</span> Bayes Factors</h2>
<p>We can use Bayesian Inference through a tool known as Bayes Factors. Bayes Factors are a method of directly comparing the posteriors of different models with different evidences and priors.</p>
<p>Bayes Factors make a ratio of the result of one model or hypothesis over another, resulting in a single quantity that we can examine. Consider that our hypotheses above have been put through the process and a result gained thus</p>
<ul>
<li><span class="math inline">\(H_0 : \bar{x}_1 &lt; \bar{x}_2 = 0.3\)</span></li>
<li><span class="math inline">\(H_1 : \bar{x}_1 &gt; \bar{x}_2 = 0.9\)</span></li>
</ul>
<p>We can clearly see that <span class="math inline">\(H_1\)</span> has 3 times more support than <span class="math inline">\(H_0\)</span> and we would want to accept that as a better explanation of our data.</p>
<p>Bayes Factors are just that, the ratio of the relative goodness of the hypotheses. From this we can make statements about the support for hypotheses. <span class="citation">Wagenmakers et al. (<a href="#ref-wagenmakers2011" role="doc-biblioref">2011</a>)</span> created a table of thresholds indicating interpretations for different Bayes Factors on two hypotheses.</p>
<table>
<thead>
<tr class="header">
<th align="left">Bayes.Factor</th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">&gt;100</td>
<td align="left">Extreme evidence for <span class="math inline">\(H_0\)</span> compared to <span class="math inline">\(H_1\)</span></td>
</tr>
<tr class="even">
<td align="left">30..100</td>
<td align="left">Very Strong evidence for <span class="math inline">\(H_0\)</span> compared to <span class="math inline">\(H_1\)</span></td>
</tr>
<tr class="odd">
<td align="left">10..30</td>
<td align="left">Strong evidence for <span class="math inline">\(H_0\)</span> compared to <span class="math inline">\(H_1\)</span></td>
</tr>
<tr class="even">
<td align="left">3..10</td>
<td align="left">Substantial evidence for <span class="math inline">\(H_0\)</span> compared to <span class="math inline">\(H_1\)</span></td>
</tr>
<tr class="odd">
<td align="left">1..3</td>
<td align="left">Anecdotal evidence for <span class="math inline">\(H_0\)</span> compared to <span class="math inline">\(H_1\)</span></td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">No evidence</td>
</tr>
<tr class="odd">
<td align="left">1..1/3</td>
<td align="left">Anecdotal evidence for <span class="math inline">\(H_1\)</span> compared to <span class="math inline">\(H_0\)</span></td>
</tr>
<tr class="even">
<td align="left">1/3..1/10</td>
<td align="left">Substantial evidence for <span class="math inline">\(H_1\)</span> compared to <span class="math inline">\(H_0\)</span></td>
</tr>
<tr class="odd">
<td align="left">1/10..1/30</td>
<td align="left">Strong evidence for <span class="math inline">\(H_1\)</span> compared to <span class="math inline">\(H_0\)</span></td>
</tr>
<tr class="even">
<td align="left">1/30..1/100</td>
<td align="left">Very Strong evidence for <span class="math inline">\(H_1\)</span> compared to <span class="math inline">\(H_0\)</span></td>
</tr>
<tr class="odd">
<td align="left">&lt;1/100</td>
<td align="left">Extreme evidence for <span class="math inline">\(H_1\)</span> compared to <span class="math inline">\(H_0\)</span></td>
</tr>
</tbody>
</table>
<p>These are extremely useful especially when used with other measures and interpretations like estimation statistics to allow us to make statistical claims.</p>
<p>In the next chapters we will look at how to use Bayes Factors in place of common frequentist hypothesis tests.</p>

<div class="sidenote">
<p>The <span class="citation">Wagenmakers et al. (<a href="#ref-wagenmakers2011" role="doc-biblioref">2011</a>)</span> article is fun if you can get hold of it. It’s a commentary on an earlier article in which the researchers conclude that people have the ability to see into the future by misunderstanding and misapplying statistics. Wagenmakers <em>et al</em> reperform the statistics with Bayes Factors and show that the original conclusions are unsound.</p>
</div>


</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-wagenmakers2011">
<p>Wagenmakers, E. J., T. Wetzels, D. Borsboom, and H. L. J van der Maas. 2011. “Why Psychologists Must Change the Way They Analyze Their Data: The Case of Psi: Comment on Bem (2011).” <em>Journal of Personality and Social Psychology</em> 100 (3): 426–32. <a href="https://doi.org/https://doi.org/10.1037/a0022790">https://doi.org/https://doi.org/10.1037/a0022790</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="r-fundamentals.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayes-factor-t-tests.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bayes_factors.pdf", "bayes_factors.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
